import os
import numpy as np
from torch import nn
from torch.nn import init
from torch.nn.functional import elu

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Silences the warning and error logs generated by TensorFlow

import matplotlib.pyplot as plt
from keras.optimizers import Adam
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Flatten, LeakyReLU, Reshape, BatchNormalization


class Ensure4d(nn.Module):
    def forward(self, x):
        while(len(x.shape) < 4):
            x = x.unsqueeze(-1)
        return x


class Expression(nn.Module):
    """Compute given expression on forward pass.

    Parameters
    ----------
    expression_fn : callable
        Should accept variable number of objects of type
        `torch.autograd.Variable` to compute its output.
    """

    def __init__(self, expression_fn):
        super(Expression, self).__init__()
        self.expression_fn = expression_fn

    def forward(self, *x):
        return self.expression_fn(*x)

    def __repr__(self):
        if hasattr(self.expression_fn, "func") and hasattr(
            self.expression_fn, "kwargs"
        ):
            expression_str = "{:s} {:s}".format(
                self.expression_fn.func.__name__, str(self.expression_fn.kwargs)
            )
        elif hasattr(self.expression_fn, "__name__"):
            expression_str = self.expression_fn.__name__
        else:
            expression_str = repr(self.expression_fn)
        return (
            self.__class__.__name__ +
            "(expression=%s) " % expression_str
        )


class Generator(nn.Sequential):
    def __init__(
            self,
            channels=1,
            time_stamp=1000,
            noise=100,
            dropout=0.2,
            alpha=0.2,
            momentum=0.8):

        super().__init__()

        # Dataset features:
        self.channels = channels
        self.time_stamp = time_stamp
        self.eeg_shape = (self.time_stamp, self.channels)

        # Model specific parameters (Noise generation, Dropout for overfitting reduction, etc...):
        self.noise = noise
        self.dropout = dropout
        self.alpha = alpha
        self.momentum = momentum

        self.add_module("ensuredims", Ensure4d())
        self.add_module(nn.Linear(self.noise, 256))
        self.add_module(nn.LeakyReLU(self.alpha))
        self.add_module("bnorm", nn.BatchNorm2d(momentum=self.momentum))
        self.add_module(nn.Linear(256, 512))
        self.add_module(nn.LeakyReLU(self.alpha))
        self.add_module("bnorm", nn.BatchNorm2d(momentum=self.momentum))
        self.add_module(nn.Linear(512, 1024))
        self.add_module(nn.LeakyReLU(self.alpha))
        self.add_module("bnorm", nn.BatchNorm2d(momentum=self.momentum))
        self.add_module(nn.Tanh())

        self.add_module("squeeze", Expression(self.eeg_shape))
        # Start in eval mode
        self.eval()

        # # Choosing Adam optimiser for both generator and discriminator to feed in to the model:
        # self.optimiser = Adam(0.0002, 0.2)  # Values from the EEG GAN paper found to be most optimal
        #
        # # Builds the Generator, Discriminator and the combined models:
        # self.generator = self.make_generator()
        #
        # self.discriminator = self.make_discriminator()
        # self.discriminator.compile(loss='binary_crossentropy',
        #                            optimizer=self.optimiser,
        #                            metrics=['accuracy'])
        #
        # self.combined = self.builder()
        # self.combined.compile(loss='binary_crossentropy',
        #                       optimizer=self.optimiser)
        #
        # # Useful for creating a sample directory later
        # self.dir = 'EEG_samples'

    def make_generator(self):
        '''
        Creates a generator model that takes in randomly generated noise, then uses
        3 Dense layers to return an image that is fed into the discriminator,
        which then distinguishes whether or not it is a real or fake one. Weights are adjusted
        accordingly such that it can eventually generate a real signal.
        :return: Model data tuple (generated noise sample, eeg img)
        '''
        model = Sequential()

        model.add(Dense(256, input_dim=self.noise))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(Dense(np.prod(self.eeg_shape), activation='tanh'))
        model.add(Reshape(self.eeg_shape))
        assert model.output_shape == (None, 1000, self.channels)

        noise = Input(shape=(self.noise,))
        img = model(noise)
        print(model.summary())
        return Model(noise, img)


class Discriminator(nn.Sequential):
    def __init__(
            self,
            channels=1,
            time_stamp=1000,
            noise=100,
            dropout=0.2,
            alpha=0.2,
            momentum=0.8):
        super().__init__()

        # Dataset features:
        self.channels = channels
        self.time_stamp = time_stamp
        self.eeg_shape = (self.time_stamp, self.channels)

        # Model specific parameters (Noise generation, Dropout for overfitting reduction, etc...):
        self.noise = noise
        self.dropout = dropout
        self.alpha = alpha
        self.momentum = momentum

        model.add(Flatten(input_shape=self.eeg_shape))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dense(1, activation='sigmoid'))

        self.add_module("squeeze", Expression(self.eeg_shape))
        # Start in eval mode
        self.eval()

        # # Choosing Adam optimiser for both generator and discriminator to feed in to the model:
        # self.optimiser = Adam(0.0002, 0.2)  # Values from the EEG GAN paper found to be most optimal
        #
        # # Builds the Generator, Discriminator and the combined models:
        # self.generator = self.make_generator()
        #
        # self.discriminator = self.make_discriminator()
        # self.discriminator.compile(loss='binary_crossentropy',
        #                            optimizer=self.optimiser,
        #                            metrics=['accuracy'])
        #
        # self.combined = self.builder()
        # self.combined.compile(loss='binary_crossentropy',
        #                       optimizer=self.optimiser)
        #
        # # Useful for creating a sample directory later
        # self.dir = 'EEG_samples'

    def make_generator(self):
        '''
        Creates a generator model that takes in randomly generated noise, then uses
        3 Dense layers to return an image that is fed into the discriminator,
        which then distinguishes whether or not it is a real or fake one. Weights are adjusted
        accordingly such that it can eventually generate a real signal.
        :return: Model data tuple (generated noise sample, eeg img)
        '''
        model = Sequential()

        model.add(Dense(256, input_dim=self.noise))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(Dense(np.prod(self.eeg_shape), activation='tanh'))
        model.add(Reshape(self.eeg_shape))
        assert model.output_shape == (None, 1000, self.channels)

        noise = Input(shape=(self.noise,))
        img = model(noise)
        print(model.summary())
        return Model(noise, img)

    def make_discriminator(self):
        '''
        Creates a discriminator model that distingushes the fed images from generator,
        and also is trained using a training loop (see below). The Discriminator is a simple
        2 Dense layer Neural Netowrk that returns either a 'True' or 'False'. Values are then adjusted accordingly
        per epoch to update weights and biases such that it produces the right output (i.e. it can
        discriminate fake from real).
        :return: Model data tuple (image from generator, validity [true or false])
        '''
        model = Sequential()

        model.add(Flatten(input_shape=self.eeg_shape))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dense(1, activation='sigmoid'))

        img = Input(shape=self.eeg_shape)
        validity = model(img)
        print(model.summary())
        return Model(img, validity)
