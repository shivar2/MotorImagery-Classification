import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Silences the warning and error logs generated by TensorFlow

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from keras.losses import BinaryCrossentropy
from keras.models import Sequential
from keras.layers import Dense, Flatten, LeakyReLU, Dropout
from keras.layers import Reshape, BatchNormalization, Conv2D, Conv2DTranspose

class DCGAN:
    def __init__(self, channels=1, batchsize=50):

        # Dataset features:
        self.channels = channels
        self.freq_sample = 25
        self.time_sample = 342
        self.eeg_shape = (self.freq_sample, self.time_sample, self.channels)

        # Model specific parameters (Noise generation, Dropout for overfitting reduction, etc...):
        self.noise = 100
        self.dropout = 0.25
        self.alpha = 0.2
        self.momentum = 0.8
        self.batchsize = batchsize

        # Choosing Adam optimiser for both generator and discriminator to feed in to the model:
        self.optimiser = Adam(0.0002, 0.2) # Values from the EEG GAN paper found to be most optimal

        # Build both the Generator and Discriminator:
        # We will train the combined model this time, unlike standard GAN
        self.generator = self.make_generator()
        self.discriminator = self.make_discriminator()

        # Useful for creating a sample directory later
        self.dir = 'EEG_samples'


    def make_generator(self):
        '''
        Creates a generator model that takes in randomly generated noise, then uses
        3 upsampling layers to return an image that is fed into the discriminator
        which then distinguishes whether or not it is a real or fake one. Weights are adjusted
        accordingly such that it can eventually generate a real signal.
        :return:
        '''

        model = Sequential()

        model.add(Dense(4 * 41 * 256, use_bias=False, input_shape=(self.noise,)))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(LeakyReLU())

        model.add(Reshape((4, 41, 256)))

        model.add(Conv2DTranspose(128, (5, 4), strides=(2, 2), padding='valid', use_bias=False))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(LeakyReLU())

        model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='valid', use_bias=False))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(LeakyReLU())

        model.add(Conv2DTranspose(self.channels, (5, 5), strides=(1, 2), padding='same', use_bias=False,
                                         activation='tanh')) # Using tanh for output also based on the EEG paper
        assert model.output_shape == (None, 25, 342, self.channels)

        # Prints a small summary of the network
        # model.summary()

        return model

    def make_discriminator(self):
        '''
        Creates a discriminator model that distingushes the fed images from generator,
        and also is trained using a training loop (see below). The Discriminator is a simple
        2 layer CNN that returns either a 'True' or 'False'. Values are then adjusted accordingly
        per epoch to update weights and biases such that it produces the right output (i.e. it can
        discriminate fake from real).
        :return:
        '''

        model = Sequential()
        model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                input_shape=[25, 342, self.channels]))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dropout(self.dropout))

        model.add(Conv2D(128, (5, 5), strides=(1, 2), padding='same'))
        model.add(BatchNormalization(momentum=self.momentum))
        model.add(LeakyReLU(alpha=self.alpha))
        model.add(Dropout(self.dropout))

        model.add(Flatten())
        model.add(Dense(1))
        assert model.output_shape == (None, 1)

        # Prints a small summary of the network
        # model.summary()

        return model

    def make_fakedata(self, noise_shape=100):
        '''
        Generates the fake data by drawing random samples from
        a normal Gaussian distribution (which is what np.random.normal
        does). This is for the generator to use.
        :return: Generated signal, Noise np.array
        '''

        noise = np.random.normal(0, 1, (noise_shape, self.noise))
        gen_imgs = self.generator.predict(noise)
        return gen_imgs, noise


    def discriminator_loss(self, real_output, fake_output):
        '''
        Defines the loss function for the descriminator.
        Uses cross entropy a.k.a (log-loss) helper function from
        Keras 'BinaryCrossEntropy'. Returns the combined loss
        '''
        cross_entropy = BinaryCrossentropy(from_logits=True)
        real_loss = cross_entropy(tf.ones_like(real_output), real_output) # Zero output for real
        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) # One output for fake
        total_loss = real_loss + fake_loss
        return total_loss

    def generator_loss(self, fake_output):
        '''
        Like the above but this time for generator...
        :return: Generator loss value
        '''

        cross_entropy = BinaryCrossentropy(from_logits=True)
        return cross_entropy(tf.ones_like(fake_output), fake_output) # TF array of ones for real output

    @tf.function
    def train_step(self, images):
        '''
        This training step function that follows from the official TensorFlow documentation.
        It is in the form of tf.function which allows it to be compiled, rather than
        compiling the combined models alone everytime. More specificially, it makes use
        of GradientTape() function to train both generator and discriminator separately.
        :return: Discriminator and Generator loss
        '''

        # GradientTape allows us to do automatic differentiation handled by TensorFlow
        # Useful when doing back propagation obviously. It also watches all the differentiable
        # Variables

        noise = tf.random.normal([self.batchsize, self.noise])

        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
            generated_images = self.generator(noise, training=True)

            real_output = self.discriminator(images, training=True)
            fake_output = self.discriminator(generated_images, training=True)

            gen_loss = self.generator_loss(fake_output)
            disc_loss = self.discriminator_loss(real_output, fake_output)

        grad_gen = gen_tape.gradient(gen_loss, self.generator.trainable_variables)
        grad_disc = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)

        self.optimiser.apply_gradients(zip(grad_gen, self.generator.trainable_variables))
        self.optimiser.apply_gradients(zip(grad_disc, self.discriminator.trainable_variables))


        return disc_loss, gen_loss

    def train(self, dataset, epochs, sample_interval=100):

        '''
        The training function that has a loop which trains the model on
        every epoch/iteration. Calls the train_step() compiled function
        which trains the combined model at the same time.
        '''

        # Allows us to 'unpack' our dataset using .from_tensor_slices, shuffling it
        # and also batching it.

        gen_loss, disc_loss = [], []
        g_tot, d_tot = [], []

        data = tf.data.Dataset.from_tensor_slices(dataset.astype('float32'))\
                .shuffle(dataset.shape[0]).batch(self.batchsize)

        for epoch in range(epochs):

            for image_batch in data:
                disc_loss_batch, gen_loss_batch = self.train_step(image_batch)

                gen_loss.append(gen_loss_batch)
                disc_loss.append(disc_loss_batch)

            g_loss = sum(gen_loss)/len(gen_loss)
            d_loss = sum(disc_loss)/len(disc_loss)

            g_tot.append(g_loss)
            d_tot.append(d_loss)

            if epoch % sample_interval == 0:
                print("epoch: {}, generator loss: {}, discriminator loss: {}".format
                    (epoch, g_loss, d_loss))

                # Allows us to generate the signal and get the fake one for a
                # Arbitrary trial number. Plots it and save it every sample_interval
                # Which is 100 in this case.
                generated_signal, _ = self.make_fakedata(noise_shape=100)
                trial_num, channel = 30, 0
                real_signal = np.expand_dims(dataset[trial_num], axis=0)

                # Plots the generated samples for the selected channels.
                # Recall the channels are chosen during the Load_and_Preprocess Script
                # Here they just correspond to C3 only (channel 7 was selected).
                fig, axs = plt.subplots(1, 2)
                fig.suptitle('Comparison of Generated vs. Real Signal (Spectrogram) for one trial, one channel')
                fig.tight_layout()
                axs[0].imshow(generated_signal[0, :, :, channel], aspect='auto')
                axs[0].set_title('Generated Signal', size=10)
                axs[0].set_xlabel('Time Sample')
                axs[0].set_ylabel('Frequency Sample')
                axs[1].imshow(real_signal[0, :, :, channel], aspect='auto')
                axs[1].set_title('Fake Signal', size=10)
                axs[1].set_xlabel('Time Sample')
                axs[1].set_ylabel('Frequency Sample')
                plt.show()

                # Save the generated samples within the current working dir
                # in a folder called 'EEG Samples', every 100 epochs.
                if not os.path.exists(self.dir):
                    os.makedirs(self.dir)

                plt.savefig("%s/%d.png" % (self.dir, epoch))
                plt.close()

        # Plot the generator and discriminator losses for all the epochs
        plt.figure()
        plt.plot(g_tot, 'r')
        plt.plot(d_tot, 'b')
        plt.title('Loss history')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend(['Generator', 'Discriminator'])
        plt.grid()
        plt.show()


# Load the data and store it appropriately

train_dataset_task1 = np.load('Subject1Train1.npz')

print(train_dataset_task1.get('y'))

train_data = train_dataset_task1['x']
train_labels = train_dataset_task1['y']

test = DCGAN()
test.train(train_data, epochs=2500)


'''
                # store history
                gen_loss += gen_loss_batch / float(N_batch)
                disc_loss += disc_loss_batch / float(N_batch)
                gen_grads += gen_grads_batch / float(N_batch)
                disc_grads += disc_grads_batch / float(N_batch)


            # store history
            gen_loss_history.append(gen_loss)
            disc_loss_history.append(disc_loss)
            gen_grads_history.append(gen_grads)
            disc_grads_history.append(disc_grads)
'''