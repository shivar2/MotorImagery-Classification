{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_CWT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eLY22niMOaj",
        "colab_type": "text"
      },
      "source": [
        "##**CNN Training on CWT data**\n",
        "In this notebook, we instantiate a CNN model to train and test on the EEG data represented through a CWT (continuous wavelet transform). We chose this representation of the data due to the fact that the GAN, which will output artificial data, learns better on frequency vs. time data. We experiment on the natural data only on one subject with 5 EEGs due to RAM constraints of the Google Colab capabilites. \n",
        "\n",
        "The artificial data is appended in ratios of 25%, 50%, and 100% and is compared to the baseline of 0% appended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajP1Qwu53EOC",
        "colab_type": "code",
        "outputId": "66f69088-92af-41d9-b58d-49dcada1e457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_aLZ0ZW4ZFj",
        "colab_type": "code",
        "outputId": "85dd6565-42c7-4148-a614-b9188915c149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# change path\n",
        "import os\n",
        "\n",
        "try:\n",
        "  project_fname = '/content/drive/My Drive/Colab Notebooks/c247/'\n",
        "  os.chdir(project_fname)\n",
        "except:\n",
        "  project_fname='/content/drive/My Drive/c247'\n",
        "  os.chdir(project_fname)\n",
        "project_data_path = os.path.join(project_fname,'project_data/')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/c247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHz5sjkWzkUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "import numpy as np\n",
        "import random\n",
        "from load_data import * \n",
        "from data_preprocessing import * \n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "\n",
        "X_test, y_test, person_train_valid, X_train_valid, y_train_valid, person_test = load_data(dir_path = project_data_path)\n",
        "\n",
        "# normalize the data\n",
        "N_trials,N_eeg,N_bins,_ = X_train_valid.shape\n",
        "X_train_valid_norm = np.reshape(preprocessing.scale(np.reshape(X_train_valid,(N_trials*N_eeg,N_bins)),axis=1),(N_trials,N_eeg,N_bins,1))\n",
        "N_trials,N_eeg,N_bins,_ = X_test.shape\n",
        "X_test_norm = np.reshape(preprocessing.scale(np.reshape(X_test,(N_trials*N_eeg,N_bins)),axis=1),(N_trials,N_eeg,N_bins,1))\n",
        "\n",
        "\n",
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYQPBHX0H8LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from scipy import signal \n",
        "import operator\n",
        "\n",
        "# Data preprocessing\n",
        "\n",
        "# Subsample and split into 1 person and 5 EEGs\n",
        "subsample = 5\n",
        "subj = [5]\n",
        "eegs = [0,7,9,11,19]\n",
        "\n",
        "X_train_valid_subsample, y_train_valid_subsample, person_train_valid_subsample = subsample_data(X_train_valid_norm,y_train_valid, person_train_valid, sample_every=subsample)\n",
        "X_train, y_train, person_train = split_data_by_subject(X_train_valid_subsample, y_train_valid_subsample, person_train_valid_subsample)\n",
        "\n",
        "#only get wanted subjects from X_train rather than have all subjects\n",
        "X_train = operator.itemgetter(*subj)(X_train)\n",
        "y_train = operator.itemgetter(*subj)(y_train)\n",
        "\n",
        "X_train = X_train[:,eegs,:,:]\n",
        "\n",
        "print('Shapes: x = {}, y = {}'.format(X_train.shape, y_train.shape))\n",
        "\n",
        "# wavelet transform \n",
        "N_trials,N_eeg,N_bins,_ = X_train.shape\n",
        "fs = 250\n",
        "freq_bins = 50\n",
        "X_train_cwt = morlet_wavelet_transform(X_train,fs=fs,freq_range=(1,20),freq_bins=freq_bins,w=6)\n",
        "# reshape to input to CNN\n",
        "X_train_cwt = np.swapaxes(np.swapaxes(X_train_cwt,1,3),1,2)\n",
        "# scale between -1 and 1 too mimic output of generator\n",
        "X_train_cwt_norm = 2 * (X_train_cwt - np.min(X_train_cwt,axis=0) ) / (np.max(X_train_cwt,axis=0) - np.min(X_train_cwt,axis=0)) - 1\n",
        "\n",
        "print('X_train_cwt_norm = {}'.format(X_train_cwt_norm.shape))\n",
        "\n",
        "print('Deleting original training/validation data from memory...')\n",
        "del X_train_valid_subsample, y_train_valid_subsample, person_train_valid_subsample,\\\n",
        "    X_train, person_train, X_train_valid, y_train_valid, person_train_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_yaZdeLnY1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the artificial data and append all of it to the training data\n",
        "split_art_data = 4\n",
        "for task in range(4):\n",
        "  pickle_file = os.path.join(project_data_path,'X_artificial_task{}_subj5_eegs5.npy'.format(task))\n",
        "  trials = np.load(pickle_file)\n",
        "  N=trials.shape[0]\n",
        "  X_train_cwt_norm = np.append(X_train_cwt_norm,trials[:N//split_art_data],axis=0)\n",
        "  y_train = np.append(y_train,np.ones((N//split_art_data))*task)\n",
        "print(X_train_cwt_norm.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nazkH5iNI00d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert test data with CWT exactly the same as X_train_valid\n",
        "\n",
        "X_test_subsample, y_test_subsample, person_test_subsample = subsample_data(X_test_norm,y_test, person_test, sample_every=subsample)\n",
        "X_test, y_test, person_test = split_data_by_subject(X_test_subsample, y_test_subsample, person_test_subsample)\n",
        "\n",
        "#get the specific subjects we want by indexing with the operator module\n",
        "X_test = operator.itemgetter(*subj)(X_test)\n",
        "y_test = operator.itemgetter(*subj)(y_test)\n",
        "X_test = X_test[:,eegs,:,:]\n",
        "\n",
        "# wavelet transform \n",
        "N_trials,N_eeg,N_bins,_ = X_test.shape\n",
        "fs = 250\n",
        "freq_bins = 50\n",
        "X_test_cwt = morlet_wavelet_transform(X_test,fs=fs,freq_range=(1,20),freq_bins=freq_bins,w=6)\n",
        "# reshape to put in same format as the WGAN output\n",
        "X_test_cwt = np.swapaxes(np.swapaxes(X_test_cwt,1,3),1,2)\n",
        "# scale between -1 and 1\n",
        "X_test_cwt_norm = 2 * (X_test_cwt - np.min(X_test_cwt,axis=0) ) / (np.max(X_test_cwt,axis=0) - np.min(X_test_cwt,axis=0)) - 1\n",
        "\n",
        "print('X_cwt_norm = {}'.format(X_test_cwt_norm.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G-SCgVwt7as",
        "colab_type": "text"
      },
      "source": [
        "Connect to Google TPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BqZ-X4S3cnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sOk3iY73ffu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import all needed modules\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Input,Dense,Conv2D,ReLU,ELU,\\\n",
        "  Activation,Flatten,AveragePooling2D,Softmax,BatchNormalization,MaxPooling2D,\\\n",
        "  Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,\\\n",
        "  ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAuUEai9t4B4",
        "colab_type": "text"
      },
      "source": [
        "Set up the layers for the ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlHLV3CX3h5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#(50,200,22)\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=32,\n",
        "                   kernel_size=(7,7),\n",
        "                   data_format='channels_last',\n",
        "                   kernel_regularizer=regularizers.l2(0.01),\n",
        "                   activation='relu',\n",
        "                   kernel_initializer='lecun_uniform',\n",
        "                   input_shape=(50,200,5))) #(42,192,32)\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(MaxPooling2D())#(21,96,32)\n",
        "  model.add(Dropout(rate=0.5))\n",
        "  model.add(Conv2D(filters=32,\n",
        "                   kernel_size=(7,7),\n",
        "                   data_format='channels_last',\n",
        "                   kernel_regularizer=regularizers.l2(0.01),\n",
        "                   activation='relu',\n",
        "                   kernel_initializer='lecun_uniform'))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(MaxPooling2D()) \n",
        "  model.add(Dropout(rate=0.5))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(750,\n",
        "                  kernel_regularizer=regularizers.l2(0.01),\n",
        "                  activation='relu',\n",
        "                  kernel_initializer='lecun_uniform'))\n",
        "  model.add(Dense(num_classes,\n",
        "                  activation='softmax'))\n",
        "  return model\n",
        "\n",
        "rand_seed = int(datetime.strftime(datetime.now(),\"%Y%m%d%H%M%S\"))\n",
        "batch_size=32\n",
        "num_folds=5\n",
        "num_classes=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNEvA3pBt1-R",
        "colab_type": "text"
      },
      "source": [
        "Compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCSVzIIGV-yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(learning_rate=1e-5)\n",
        "try:\n",
        "  del model\n",
        "  K.clear_session()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "model = create_model()\n",
        "model.compile(loss=categorical_crossentropy,optimizer=optimizer,metrics=['accuracy'])\n",
        "#plot_model(model,'cnn_cwt_architecture.png',show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N6K9li3mmHX",
        "colab_type": "text"
      },
      "source": [
        "Here we overfit our model on the entire training set over 30 epochs. We test if the additional artificial data makes the model more generalizable. We set the test set as the validation data to see how it performs on the test set over the epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGcGAjzk_2NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#lr_plateau = ReduceLROnPlateau(patience=4)\n",
        "history = model.fit(x=X_train_cwt_norm,\n",
        "          y=to_categorical(y_train),\n",
        "          epochs=30,\n",
        "          #validation_split=1/num_folds,\n",
        "          validation_data=(X_test_cwt_norm,to_categorical(y_test)),\n",
        "          batch_size=batch_size,\n",
        "          callbacks=[])\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}