import numpy as np
import os
import matplotlib.pyplot as plt
from keras import regularizers
from keras.optimizers import Adam
from keras.models import Sequential, load_model
from keras.layers import BatchNormalization, Conv2D, Flatten, Dense, Dropout, MaxPooling2D
from keras.losses import categorical_crossentropy
from keras.utils import to_categorical

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Silences the warning and error logs generated by TensorFlow

import tensorflow as tf

def create_artificial_trials(subject=1, task=1, sample_num=100):

    # Loads the initial model data
    fp = os.path.join(os.getcwd(), 'EEG_Samples')
    load_generator = load_model(os.path.join(fp, 'Subject{}WGAN_Model_Data_For_Task{}.h5'.format(subject, task)))

    # Generates 100 fake trial samples
    noise = tf.random.normal([sample_num, 100])
    gen_signals = load_generator(noise, training=False).numpy()

    # Saves the generated samples into a numpy array '.npy'
    fname = os.path.join(fp, 'Subject{}_Artificial_Data_For_Task{}.npy'.format(subject,task))
    np.save(fname, gen_signals)

# The following next few lines loads all of the training data (and testing) for one subject and
# Concatenates them into one big training array for one subject

def load_training_data():

    train_dataset_task1a = np.load('D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\Subject1Train1.npz')
    train_dataset_task1b = np.load('D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\Subject1Train2.npz')
    train_dataset_task1c = np.load('D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\Subject1Train3.npz')

    train_data = train_dataset_task1a['x']
    train_labels = train_dataset_task1a['y']

    train_data2 = train_dataset_task1b['x']
    train_labels2 = train_dataset_task1b['y']

    train_data3 = train_dataset_task1c['x']
    train_labels3 = train_dataset_task1c['y']

    train_data1 = np.concatenate((train_data, train_data2),axis=0)
    train_data_total = np.concatenate((train_data1, train_data3), axis=0)

    train_label1 = np.concatenate((train_labels, train_labels2),axis=0)
    train_label_total = np.concatenate((train_label1, train_labels3), axis=0)
    print(train_label_total.shape)

    print(train_label_total)
    print(train_data_total.shape)

    return train_data_total, train_label_total

def load_validation_data():
    '''
    Use the previously created test data during Load_and_Process for validation
    :return: Test_data, Test_labels
    '''
    test_dataset_task1a = np.load('D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\Subject1Test1.npz')
    test_dataset_task1b = np.load('D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\Subject1Test2.npz')
    test_dataset_task1c = np.load('D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\Subject1Test3.npz')

    test_data = test_dataset_task1a['x']
    test_labels = test_dataset_task1a['y']

    test_data2 = test_dataset_task1b['x']
    test_labels2 = test_dataset_task1b['y']

    test_data3 = test_dataset_task1c['x']
    test_labels3 = test_dataset_task1c['y']

    test_data1 = np.concatenate((test_data, test_data2), axis=0)
    test_data_total = np.concatenate((test_data1, test_data3), axis=0)

    test_label1 = np.concatenate((test_labels, test_labels2), axis=0)
    test_label_total = np.concatenate((test_label1, test_labels3), axis=0)

    return test_data_total, test_label_total

# Create the CNN classifier
def create_model():
  model = Sequential()
  model.add(Conv2D(filters=32,kernel_size=(7,7), data_format='channels_last',kernel_regularizer=regularizers.l2(0.01),
                   activation='relu', kernel_initializer='lecun_uniform', input_shape=(25,342,1)))
  model.add(BatchNormalization(axis=-1))
  model.add(MaxPooling2D())
  model.add(Dropout(rate=0.5))
  model.add(Conv2D(filters=32, kernel_size=(7,7), data_format='channels_last', kernel_regularizer=regularizers.l2(0.01),
                   activation='relu', kernel_initializer='lecun_uniform'))
  model.add(BatchNormalization(axis=-1))
  model.add(MaxPooling2D())
  model.add(Dropout(rate=0.5))
  model.add(Flatten())
  model.add(Dense(768, kernel_regularizer=regularizers.l2(0.01), activation='relu', kernel_initializer='lecun_uniform'))
  model.add(Dense(4, activation='softmax')) # <- 4 is NOT AN ERROR! According to the documentations, it always takes
                                            # Number of labels + 1 as an output if you use 'to_categorical'!
                                            # https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical
                                            # Need to look into integer encoding or other methods for the future...
  return model

model = create_model()
model.compile(loss=categorical_crossentropy, optimizer=Adam(0.00005), metrics=['accuracy'])

td, tl = load_training_data()
test_d, test_l = load_validation_data()

# The next lines append the artificial data onto the real data (per task):

task = 1
split_ratio = 8 # Appends 1/8th of the artificial training set

pickle_file = 'D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\EEG_samples\Subject1_Artificial_Data_For_Task{}.npy'.format(task)
artificial_trial_data = np.load(pickle_file)
N = artificial_trial_data.shape[0]
new_train_data = np.append(td, artificial_trial_data[:N//split_ratio],axis=0)
new_train_label = np.append(tl, np.ones((N//split_ratio))*task)
print(task)
print(new_train_data.shape)
print(new_train_label.shape)

print(new_train_label)
print(new_train_data[:, 0, 0, 0])

task = 2
split_ratio = 8

pickle_file = 'D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\EEG_samples\Subject1_Artificial_Data_For_Task{}.npy'.format(task)
artificial_trial_data = np.load(pickle_file)
N = artificial_trial_data.shape[0]
new_train_data = np.append(new_train_data, artificial_trial_data[:N//split_ratio],axis=0)
new_train_label = np.append(new_train_label, np.ones((N//split_ratio))*task)
print(task)
print(new_train_data.shape)
print(new_train_label.shape)

task = 3
split_ratio = 8

pickle_file = 'D:\Ciaran Python Data\Transfer-SARzWXT6VsetANd8\motorImagery\EEG_samples\Subject1_Artificial_Data_For_Task{}.npy'.format(task)
artificial_trial_data = np.load(pickle_file)
N = artificial_trial_data.shape[0]
new_train_data = np.append(new_train_data, artificial_trial_data[:N//split_ratio],axis=0)
new_train_label = np.append(new_train_label, np.ones((N//split_ratio))*task)
print(task)
print(new_train_data.shape)
print(new_train_label.shape)

# Instantiate object and fit the model data
Obj = model.fit(x=new_train_data,
          y=to_categorical(new_train_label), epochs=50, validation_data=(test_d, to_categorical(test_l)),
          batch_size=32 ,callbacks=[]) # Note how we use One hot encoding using 'To_categorical' for fitting labels

# Finally, plot the model accuracy
plt.plot(Obj.history['accuracy'])
plt.plot(Obj.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'])
plt.show()






